# Technical Challenge ğŸ”

This repository contains a technical challenge consisting of two exercises that assess web scraping/crawling, data transformation, and reporting skills.

## Exercises

### 1. Web Crawler
- ğŸ•·ï¸ Develop a crawler to extract data from **Booking**.
- ğŸ“„ Save at least **200 records** in a **CSV file**.
- â³ The crawler should be capable of scraping more data depending on runtime.

### 2. Data Transformation & Reporting
- ğŸ”„ Normalize and transform a given database into structured **CSV files**.
- ğŸ“Š Generate specific **reports** based on the transformed data.

## Requirements

To successfully complete this challenge, you need to have the following tools and libraries installed:

### **Programming Language** ğŸ’»

- ğŸ **Python 3.13.1** â€“ Ensure you have an updated version for compatibility with required libraries. Probably works with lower versions.

### **Web Crawling - Web Scraping** ğŸ•µï¸â€â™‚ï¸ 

- `Scrapy` â€“ For building and managing the web crawler efficiently.

### **Data Processing** ğŸ“–

- `Pandas` â€“ For data manipulation, cleaning, and transformation.
- `CSV` module â€“ To handle file input and output for structured datasets.

### **Reporting & Visualization** ğŸ“Š

- TBD

### **Other Dependencies**

- TBD

## Setup & Execution
1. ğŸ“¥ Clone the repository:
   ```bash
   git clone https://github.com/Villoh/inatlas-challennge.git
   cd inatlas-challenge
   ```
2. âš™ï¸ Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```
3. ğŸ•µï¸â€â™‚ï¸ Run the crawler:
   ```bash
   python crawler.py
   ```
4. ğŸ” Execute the data transformation:
   ```bash
   python transform.py
   ```